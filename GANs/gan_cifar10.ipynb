{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "import time\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=100))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(32 * 32 * 3, activation='tanh'))\n",
    "    model.add(Reshape((32, 32, 3)))\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Build and compile GAN models\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "    z = Input(shape=(100,))\n",
    "    img = generator(z)\n",
    "    validity = discriminator(img)\n",
    "    gan = Model(z, validity)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "    return gan\n",
    "\n",
    "# Function for live plot of losses\n",
    "def live_plot(losses):\n",
    "    plt.ion()\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    d_losses, g_losses = losses['D'], losses['G']\n",
    "    line1, = ax.plot(d_losses, label=\"Discriminator Loss\")\n",
    "    line2, = ax.plot(g_losses, label=\"Generator Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"GAN Training Losses\")\n",
    "    \n",
    "    while True:\n",
    "        line1.set_ydata(d_losses)\n",
    "        line2.set_ydata(g_losses)\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Function to save sample images during training\n",
    "def sample_images(epoch, generator, image_grid_rows=4, image_grid_columns=4):\n",
    "    noise = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5  # Rescale images 0 - 1\n",
    "    fig, axs = plt.subplots(image_grid_rows, image_grid_columns, figsize=(4, 4))\n",
    "    count = 0\n",
    "    for i in range(image_grid_rows):\n",
    "        for j in range(image_grid_columns):\n",
    "            axs[i, j].imshow(gen_imgs[count])\n",
    "            axs[i, j].axis('off')\n",
    "            count += 1\n",
    "    plt.show()\n",
    "\n",
    "# Function to train GAN\n",
    "def train_gan(generator, discriminator, gan, epochs, batch_size=32, sample_interval=100):\n",
    "    # Load and preprocess data\n",
    "    (X_train, _), (_, _) = cifar10.load_data()\n",
    "    X_train = (X_train - 127.5) / 127.5  # Normalize to [-1, 1]\n",
    "    half_batch = int(batch_size / 2)\n",
    "    \n",
    "    # Dictionary to store losses for live plotting\n",
    "    losses = {'D': [], 'G': []}\n",
    "    \n",
    "    # Start the live plotting in a separate thread\n",
    "    plot_thread = threading.Thread(target=live_plot, args=(losses,))\n",
    "    plot_thread.start()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Select a random half batch of real images\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        real_imgs = X_train[idx]\n",
    "        \n",
    "        # Generate fake images\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # Train the generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        valid_y = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, valid_y)\n",
    "        \n",
    "        # Store losses for live plot\n",
    "        losses['D'].append(d_loss[0])\n",
    "        losses['G'].append(g_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % sample_interval == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
    "            sample_images(epoch, generator)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
